namespace strathweb_phi_engine {
    void enable_tracing();
};

dictionary InferenceOptions {
	u16 token_count;
	f64 temperature;
	f64? top_p;
	u64? top_k;
    f32 repeat_penalty;
	u16 repeat_last_n;
	u64 seed;
};

interface InferenceOptionsBuilder {
    constructor();

    [Throws=PhiError]
    void with_token_count(u16 context_window);

    [Throws=PhiError]
    void with_temperature(f64 temperature);

    [Throws=PhiError]
    void with_top_p(f64 top_p);

    [Throws=PhiError]
    void with_top_k(u64 top_k);

    [Throws=PhiError]
    void with_repeat_penalty(f32 repeat_penalty);

    [Throws=PhiError]
    void with_repeat_last_n(u16 repeat_last_n);

    [Throws=PhiError]
    void with_seed(u64 seed);

    [Throws=PhiError]
    InferenceOptions build();
};

dictionary InferenceResult {
    string result_text;
    u16 token_count;
    f64 duration;
	f64 tokens_per_second;
};

dictionary ConversationMessage {
    Role role;
    string text;
};

dictionary ConversationContext {
    sequence<ConversationMessage> messages;
    string? system_instruction;
};

interface PhiEngine {
    [Throws=PhiError]
    InferenceResult run_inference([ByRef]string prompt_text, [ByRef]ConversationContext conversation_context, [ByRef]InferenceOptions inference_options);
};

interface StatefulPhiEngine {
    [Throws=PhiError]
    InferenceResult run_inference([ByRef]string prompt_text, [ByRef]InferenceOptions inference_options);
    
    [Throws=PhiError]
    void clear_messsages();

    [Throws=PhiError]
    sequence<ConversationMessage> get_history();
};

interface PhiEngineBuilder {
    constructor();

    [Throws=PhiError]
    void with_context_window(u16 context_window);

    [Throws=PhiError]
    void with_flash_attention(boolean use_flash_attention);

    [Throws=PhiError]
    void with_event_handler(BoxedPhiEventHandler event_handler);

    [Throws=PhiError]
    void with_model_provider(PhiModelProvider model_provider);

    [Throws=PhiError]
    void with_tokenizer_provider(TokenizerProvider tokenizer_provider);

    [Throws=PhiError]
    boolean try_use_gpu();

    [Throws=PhiError]
    PhiEngine build(string cache_dir);

    [Throws=PhiError]
    StatefulPhiEngine build_stateful(string cache_dir, string? system_instruction);
};

[Enum]
interface PhiModelProvider {
  HuggingFace(string model_repo, string model_revision);
  HuggingFaceGguf(string model_repo, string model_file_name, string model_revision);
  FileSystem(string model_path);
};

[Enum]
interface TokenizerProvider {
  HuggingFace(string tokenizer_repo, string tokenizer_file_name);
  FileSystem(string tokenizer_path);
};

enum Role {
    "Assistant",
    "User",
};

callback interface PhiEventHandler {
    [Throws=PhiError]
    void on_model_loaded();

    [Throws=PhiError]
    void on_inference_token(string token);

    [Throws=PhiError]
    void on_inference_started();

    [Throws=PhiError]
    void on_inference_ended();
};

interface BoxedPhiEventHandler {
    constructor(PhiEventHandler handler);
};

[Error]
interface PhiError {
    InitalizationError(string error_text);
    LockingError(string error_text);
    InferenceError(string error_text);
    GpuNotSupported();
};