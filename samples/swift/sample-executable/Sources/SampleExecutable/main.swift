import Foundation
import Strathweb_Phi_Engine

let (isGgufMode, isPhi4, isCpuMode) = (
    CommandLine.arguments.contains("--gguf"),
    CommandLine.arguments.contains("--phi-4"),
    CommandLine.arguments.contains("--cpu")
)

let formatMode = isGgufMode ? " üçÉ GGUF" : " üí™ Safe tensors"
let modelMode = isPhi4 ? " üöÄ Phi-4" : " üöó Phi-3"

print("\(formatMode) mode is enabled.\n\(modelMode) mode is enabled.")

let modelProvider = switch (isGgufMode, isPhi4) {
    case (true, true):
        PhiModelProvider.huggingFaceGguf(
            modelRepo: "microsoft/phi-4-gguf",
            modelFileName: "phi-4-q4.gguf",
            modelRevision: "main"
        )
    case (true, false):
        PhiModelProvider.huggingFaceGguf(
            modelRepo: "microsoft/Phi-3-mini-4k-instruct-gguf",
            modelFileName: "Phi-3-mini-4k-instruct-q4.gguf",
            modelRevision: "main"
        )
    case (false, true):
        PhiModelProvider.huggingFace(
            modelRepo: "microsoft/phi-4",
            modelRevision: "main"
        )
    case (false, false):
        PhiModelProvider.huggingFace(
            modelRepo: "microsoft/Phi-3-mini-4k-instruct",
            modelRevision: "main"
        )
}

let inferenceOptionsBuilder = InferenceOptionsBuilder()
try! inferenceOptionsBuilder.withTemperature(temperature: 0.9)
try! inferenceOptionsBuilder.withSeed(seed: 146628346)
if isPhi4 {
    try! inferenceOptionsBuilder.withChatFormat(chatFormat: ChatFormat.chatMl)
}
let inferenceOptions = try! inferenceOptionsBuilder.build()

let cacheDir = FileManager.default.currentDirectoryPath.appending("/.cache")

class ModelEventsHandler: PhiEventHandler {
    func onInferenceStarted() {
        print(" ‚ÑπÔ∏è Inference started...")
    }
    func onInferenceEnded() {
        print("\n ‚ÑπÔ∏è Inference ended.")
    }
    func onInferenceToken(token: String) {
        print(token, terminator: "")
    }
    func onModelLoaded() {
        print("""
         üß† Model loaded!
        ****************************************
        """)
    }
}

let modelBuilder = PhiEngineBuilder()
try! modelBuilder.withEventHandler(eventHandler: ModelEventsHandler())
try! modelBuilder.withModelProvider(modelProvider: modelProvider)

if isPhi4 {
    try! modelBuilder.withTokenizerProvider(tokenizerProvider: .huggingFace(
        tokenizerRepo: "microsoft/phi-4",
        tokenizerFileName: "tokenizer.json"
    ))
}

if !isCpuMode {
    let gpuEnabled = try! modelBuilder.tryUseGpu()
    print(gpuEnabled ? " üéÆ GPU mode enabled." : " üíª Tried GPU, but falling back to CPU.")
} else {
    print(" üíª CPU mode enabled.")
}

let model = try! modelBuilder.buildStateful(
    cacheDir: cacheDir,
    systemInstruction: "You are a hockey poet. Be brief and polite."
)

let result = try! model.runInference(
    promptText: "Write a haiku about ice hockey",
    inferenceOptions: inferenceOptions
)

print("""
****************************************
 üìù Tokens Generated: \(result.tokenCount)
 üñ•Ô∏è Tokens per second: \(result.tokensPerSecond)
 ‚è±Ô∏è Duration: \(result.duration)s
""")